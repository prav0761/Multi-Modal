{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "import time\n",
    "import numpy as np\n",
    "import io\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import threading\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel, OpenAIGPTLMHeadModel\n",
    "from nltk.corpus import wordnet\n",
    "from caption_transforms import SimCLRData_Caption_Transform\n",
    "from image_transforms import SimCLRData_image_Transform\n",
    "from dataset import FlickrDataset\n",
    "from models import ResNetSimCLR,OpenAI_SIMCLR\n",
    "from utils import get_gpu_stats,layerwise_trainable_parameters,count_trainable_parameters\n",
    "from metrics import ContrastiveLoss\n",
    "from metrics import LARS,Optimizer_simclr\n",
    "from logger import Logger\n",
    "from train_fns import train, test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "No of GPUs i have is 2\n",
      "0\n",
      "My Graphic Card is Tesla P100-PCIE-16GB\n",
      "Is Cuda Available True\n"
     ]
    }
   ],
   "source": [
    "get_gpu_stats()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flickr30kDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, token_file_path, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.token_file_path = token_file_path\n",
    "        self.transform = transform\n",
    "        self.captions = self._load_captions()\n",
    "\n",
    "    def _load_captions(self):\n",
    "        with open(self.token_file_path) as tokenfile:\n",
    "            captions = tokenfile.readlines()\n",
    "        caption_dict = {}\n",
    "        for caption in captions:\n",
    "            caption_parts = caption.strip().split('#')\n",
    "            image_file_name = caption_parts[0]\n",
    "            caption_text_parts = caption_parts[1].split('\\t')\n",
    "            caption_number = int(caption_text_parts[0].replace('#',''))\n",
    "            caption_text = caption_text_parts[1]\n",
    "            if image_file_name not in caption_dict:\n",
    "                caption_dict[image_file_name] = []\n",
    "            caption_dict[image_file_name].append(caption_text)\n",
    "        return caption_dict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = list(self.captions.keys())[idx]\n",
    "        image_path = os.path.join(self.root_dir, image_filename)\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        captions = self.captions[image_filename]\n",
    "        return image, captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenAI_SIMCLR_debug(nn.Module):\n",
    "    def __init__(self, model='openai-gpt', projection_dim=128,layers_to_train=['h.11'],encoder_last_layer=None):\n",
    "        super(OpenAI_SIMCLR_debug, self).__init__()\n",
    "\n",
    "        # Load backbone and tokenizer\n",
    "        self.backbone = OpenAIGPTModel.from_pretrained(model)\n",
    "        self.config = self.backbone.config\n",
    "        self.tokenizer = OpenAIGPTTokenizer.from_pretrained(model)\n",
    "        self.encoder_last_layer=encoder_last_layer\n",
    "        # Set requires_grad for each parameter based on layers_to_train\n",
    "        for name, param in self.backbone.named_parameters():\n",
    "            if any(name.startswith(prefix) for prefix in layers_to_train):\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "        if encoder_last_layer:\n",
    "            self.fc_layer=nn.Linear(self.config.n_embd,encoder_last_layer)\n",
    "            projection_head_input=encoder_last_layer\n",
    "        else:\n",
    "            projection_head_input=self.config.n_embd\n",
    "        # Projection head\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(projection_head_input ,projection_head_input),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(projection_head_input, projection_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, texts,device):\n",
    "        \n",
    "        # Tokenize input text\n",
    "        tokenized_texts = [self.tokenizer.tokenize(text) for text in texts]\n",
    "        input_ids = [self.tokenizer.convert_tokens_to_ids(tokens) for tokens in tokenized_texts]\n",
    "        tokens_tensor = pad_sequence([torch.tensor(ids) for ids in input_ids], batch_first=True, padding_value=0)\n",
    "        tokens_tensor = tokens_tensor.to(device)\n",
    "        # Get text features from backbone\n",
    "        all_hidden_states = self.backbone(tokens_tensor)\n",
    "        features = torch.mean(all_hidden_states, dim=1)  # Shape: (1, 768)\n",
    "\n",
    "        if self.encoder_last_layer:\n",
    "            features=self.fc_layer(features)\n",
    "        # Pass text features through projection head\n",
    "        projections = self.projection_head(features)\n",
    "        return features,projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\"\"\"dataset = Flickr30kDataset('/work/08629/pradhakr/maverick2/cv_project/flickr30k-images', \n",
    "                           '/work/08629/pradhakr/maverick2/cv_project/flickr30k_captions/results_20130124.token',\n",
    "                          transform=preprocess)\"\"\"\n",
    "train_dataset = FlickrDataset('data/', \"data/train\", 'train',\n",
    "                              image_transform=SimCLRData_image_Transform(),\n",
    "                              caption_transform=SimCLRData_Caption_Transform())\n",
    "\n",
    "test_dataset = FlickrDataset('data/', \"data/test\", 'test',\n",
    "                             image_transform=SimCLRData_image_Transform(),\n",
    "                             caption_transform=SimCLRData_Caption_Transform())\n",
    "# Split the dataset into train, validation, and test sets\n",
    "#train_set, val_set, test_set = torch.utils.data.random_split(dataset, [29783, 1000, 1000])\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True, \n",
    "                          num_workers=4,\n",
    "                          pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(test_dataset, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=False, \n",
    "                         num_workers=4, \n",
    "                         pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, image_model, text_model, optimizer_image, optimizer_text, criterion,device,\n",
    "          scheduler_image=None, scheduler_text=None, trade_off_ii=1, trade_off_cc=1,trade_off_ic=1,trade_off_ci=1):\n",
    "    loss_epoch = 0\n",
    "\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        image_model.train()\n",
    "        text_model.train()\n",
    "\n",
    "        batch_size = batch[0].shape[0]\n",
    "        image1, image2, caption1, caption2 = batch[0], batch[1], batch[3], batch[4]\n",
    "\n",
    "        _, embed_image1 = image_model(image1, device)\n",
    "        _, embed_image2 = image_model(image2, device)\n",
    "        _, embed_caption1 = text_model(caption1, device)\n",
    "        _, embed_caption2 = text_model(caption2, device)\n",
    "\n",
    "        contrastive_loss = (trade_off_ii * criterion(embed_image1, embed_image2, batch_size) +\n",
    "                      trade_off_cc * criterion(embed_caption1, embed_caption2, batch_size) +\n",
    "                      trade_off_ic * criterion(embed_image1, embed_caption2, batch_size) +\n",
    "                     trade_off_ci * criterion(embed_caption1, embed_image2, batch_size) )\n",
    "\n",
    "        contrastive_loss.backward()\n",
    "\n",
    "        optimizer_image.step()\n",
    "        optimizer_text.step()\n",
    "\n",
    "        optimizer_image.zero_grad()\n",
    "        optimizer_text.zero_grad()\n",
    "        \n",
    "        loss_epoch += contrastive_loss.item()\n",
    "\n",
    "        del batch, image1, image2, caption1, caption2, embed_image1, embed_image2, embed_caption1, embed_caption2, contrastive_loss\n",
    "        torch.cuda.empty_cache()\n",
    "    if scheduler_image:\n",
    "        scheduler_image.step()\n",
    "    if scheduler_text:\n",
    "        scheduler_text.step()\n",
    "    epoch_loss = loss_epoch / len(dataloader)\n",
    "    return epoch_loss\n",
    "def test(dataloader, image_model, text_model, criterion, device, trade_off_ii=1, trade_off_cc=1,trade_off_ic=1,trade_off_ci=1):\n",
    "\n",
    "    loss_epoch = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            image_model.eval()\n",
    "            text_model.eval()\n",
    "            batch_size = batch[0].shape[0]\n",
    "            image1, image2, caption1, caption2 = batch[0], batch[1], batch[3], batch[4]\n",
    "\n",
    "            _, embed_image1 = image_model(image1, device)\n",
    "            _, embed_image2 = image_model(image2, device)\n",
    "            _, embed_caption1 = text_model(caption1, device)\n",
    "            _, embed_caption2 = text_model(caption2, device)\n",
    "\n",
    "            contrastive_loss = (trade_off_ii * criterion(embed_image1, embed_image2, batch_size) +\n",
    "                      trade_off_cc * criterion(embed_caption1, embed_caption2, batch_size) +\n",
    "                      trade_off_ic * criterion(embed_image1, embed_caption2, batch_size) +\n",
    "                     trade_off_ci * criterion(embed_caption1, embed_image2, batch_size) )\n",
    "\n",
    "            loss_epoch += contrastive_loss.item()\n",
    "\n",
    "            del batch, image1, image2, caption1, caption2, embed_image1, embed_image2, embed_caption1, embed_caption2, contrastive_loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    epoch_loss = loss_epoch / len(dataloader)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ftfy or spacy is not installed using BERT BasicTokenizer instead of SpaCy & ftfy.\n"
     ]
    }
   ],
   "source": [
    "projection_dim=128\n",
    "#encoder_last_layer=2048\n",
    "image_learning_rate = 0.03\n",
    "text_learning_rate=0.03\n",
    "momentum = 0.9\n",
    "temperature = 0.07\n",
    "weight_decay = 0.0001\n",
    "optimizer_type = 'sgd'\n",
    "total_epochs=100\n",
    "trade_off_ii=1\n",
    "trade_off_cc=1\n",
    "trade_off_ic=1\n",
    "trade_off_ci=1\n",
    "model_resnet = ResNetSimCLR(\n",
    "    model='resnet50',\n",
    "    projection_dim=projection_dim,\n",
    "    layers_to_train=['layer3','layer4'],\n",
    ").to(device)\n",
    "\n",
    "# Print total number of trainable parameters of ResNetSimCLR model\n",
    "\n",
    "# Initialize OpenAI_SIMCLR model\n",
    "gpt_model = OpenAI_SIMCLR(\n",
    "    model='openai-gpt',\n",
    "    projection_dim=projection_dim,\n",
    "    layers_to_train=['h.10','h.11'],\n",
    ").to(device)\n",
    "\n",
    "# Define loss function\n",
    "NXTENT_loss = ContrastiveLoss(device, temperature=temperature)\n",
    "\n",
    "# Define optimizers and schedulers\n",
    "optimizer_image = Optimizer_simclr(optimizer_name=optimizer_type,\n",
    "                                   model_parameters=model_resnet.parameters(),\n",
    "                                   lr=image_learning_rate,\n",
    "                                   momentum=momentum,\n",
    "                                   weight_decay=weight_decay)\n",
    "\n",
    "scheduler_image = optimizer_image.scheduler\n",
    "optimizer_image = optimizer_image.optimizer\n",
    "\n",
    "optimizer_text = Optimizer_simclr(optimizer_name=optimizer_type,\n",
    "                                  model_parameters=gpt_model.parameters(),\n",
    "                                  lr=text_learning_rate,\n",
    "                                  momentum=momentum,\n",
    "                                  weight_decay=weight_decay)\n",
    "\n",
    "scheduler_text = optimizer_text.scheduler\n",
    "optimizer_text = optimizer_text.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:38<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 15.90 GiB total capacity; 5.11 GiB already allocated; 66.75 MiB free; 5.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-38bfe2526b96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                            \u001b[0mtrade_off_cc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrade_off_cc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                            \u001b[0mtrade_off_ic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrade_off_ic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                            trade_off_ci=trade_off_ci)\n\u001b[0m\u001b[1;32m     16\u001b[0m     test_loss = test(dataloader=val_loader, \n\u001b[1;32m     17\u001b[0m                      \u001b[0mimage_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_resnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8d045af10ea5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader, image_model, text_model, optimizer_image, optimizer_text, criterion, device, scheduler_image, scheduler_text, trade_off_ii, trade_off_cc, trade_off_ic, trade_off_ci)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_image1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_image2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_caption1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_caption2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cv_project/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, device)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m# Extract features from the backbone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m#x = x.unsqueeze(0) # Add batch dimension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Flatten the features and pass them through the projection head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mbn_training\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mexponential_average_factor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         )\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2282\u001b[0m     return torch.batch_norm(\n\u001b[0;32m-> 2283\u001b[0;31m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2284\u001b[0m     )\n\u001b[1;32m   2285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 15.90 GiB total capacity; 5.11 GiB already allocated; 66.75 MiB free; 5.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(100)):\n",
    "    start = time.time()\n",
    "    train_loss = train(dataloader=train_loader, \n",
    "                           image_model=model_resnet, \n",
    "                           text_model=gpt_model,\n",
    "                           optimizer_image=optimizer_image, \n",
    "                           optimizer_text=optimizer_text, \n",
    "                           criterion=NXTENT_loss,\n",
    "                            device=device,\n",
    "                           scheduler_image=scheduler_image,\n",
    "                           scheduler_text=scheduler_text,\n",
    "                           trade_off_ii=trade_off_ii, \n",
    "                           trade_off_cc=trade_off_cc,\n",
    "                           trade_off_ic=trade_off_ic,\n",
    "                           trade_off_ci=trade_off_ci)\n",
    "    test_loss = test(dataloader=val_loader, \n",
    "                     image_model=model_resnet,\n",
    "                     text_model=gpt_model,\n",
    "                     criterion=NXTENT_loss,\n",
    "                     device=device,\n",
    "                     trade_off_ii=trade_off_ii,\n",
    "                     trade_off_cc=trade_off_cc,\n",
    "                     trade_off_ic=trade_off_ic,\n",
    "                     trade_off_ci=trade_off_ci)\n",
    "    end = time.time()\n",
    "    print('trainloss',round(train_loss,3),'testloss',round(test_loss,3),'time',round(end-start,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=7\n",
    "print(a[index])\n",
    "print(b[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(b[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
